{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Early_version.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOJRlgpWBXH2QuEh+pIwcJ8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DaeSeokSong/Aengmu/blob/main/ChatBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtLYgO_OxLaU"
      },
      "source": [
        "# [ERROR 4] Korean(Hangul) breaking phenomenon Solution on Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ce18aOPbxLkY"
      },
      "source": [
        "\"\"\" \n",
        "ERROR 4. plot \"Korean\" breaking phenomenon\n",
        "- Solution: Installing(↓) and setting(plt.rc('font', family='NanumBarunGothic')) Nanum font, after runtime restart\n",
        "\"\"\"\n",
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96hYYUFzcaxY"
      },
      "source": [
        "# Google Drive Local Mount"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHqDHk2ucc7M"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgHeq1Zcb7M2"
      },
      "source": [
        "# Install Morpheme analyzer\n",
        "*   [Reference](https://soohee410.github.io/compare_tagger)\n",
        "*   [Install](https://sanghyu.tistory.com/170)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtYcWqVsb5Xk"
      },
      "source": [
        "# okt, komoran, kkma\n",
        "# install konlpy (okt, komoran, kkma)\n",
        "%%bash\n",
        "apt-get update\n",
        "apt-get install g++ openjdk-8-jdk python-dev python3-dev\n",
        "pip3 install JPype1\n",
        "pip3 install konlpy\n",
        "\n",
        "# mecab (take a long time)\n",
        "# set env\n",
        "%env JAVA_HOME \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "# install konlpy (mecab)\n",
        "%%bash\n",
        "bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
        "pip3 install /tmp/mecab-python-0.996"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GobPxVERuegZ"
      },
      "source": [
        "# Error Solution\n",
        "\n",
        "1. TypeError: startJVM() got an unexpected keyword argument 'convertStrings' [(JVM)](https://gyulogs.tistory.com/130)\n",
        "2. NameError: name 'Tagger' is not defined [(Mecab)](https://sosomemo.tistory.com/31)\n",
        "3. ParserError: Error tokenizing data. C error [(Pandas)](https://mskim8717.tistory.com/82)\n",
        "4. plot \"Korean\" breaking phenomenon on Colab [(Matplotlib)](https://teddylee777.github.io/colab/colab-korean)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpM2dDEuuesD"
      },
      "source": [
        "\"\"\" \n",
        "ERROR 1. TypeError: startJVM() got an unexpected keyword argument 'convertStrings'\n",
        "- Solution: /usr/local/lib/python3.7/dist-packages/konlpy/jvm.py, 67 line (convertStrings=True) comments processing and save jvm.py before import pakage\n",
        "\"\"\"\n",
        "\n",
        "\"\"\" \n",
        "ERROR 2. NameError: name 'Tagger' is not defined \n",
        "- Solution: Execute mecab.sh script (under code excute)\n",
        "\"\"\"\n",
        "!apt-get update\n",
        "!apt-get install g++ openjdk-8-jdk \n",
        "!pip3 install konlpy JPype1-py3\n",
        "!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cu59D-1sc93c"
      },
      "source": [
        "# Import\n",
        "*   [KoNLPy Reperence](https://konlpy-ko.readthedocs.io/ko/v0.4.3/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRaXnlVOWpot"
      },
      "source": [
        "# import for MechineLearning\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# import Morpheme analyzer\n",
        "from konlpy.tag import Kkma, Komoran, Okt, Mecab\n",
        "from konlpy.utils import pprint\n",
        "\n",
        "# import etc\n",
        "import time\n",
        "\n",
        "# Change run location\n",
        "%cd /content/drive/My Drive/DeepLearning/AI_KKH\n",
        "# Matplotlib set font on NanumBarunGothic\n",
        "plt.rc('font', family='NanumBarunGothic')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGm7Up9L5Y98"
      },
      "source": [
        "#Grobal Variable\n",
        "*   [Concept Reference](https://reniew.github.io/25/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4l77b8v5aKe"
      },
      "source": [
        "\"\"\"\n",
        "ranking about time spent morpheme analyzing (The fastest is number one.)\n",
        "\n",
        "1. Mecab\n",
        "2. Komoran\n",
        "3. Okt\n",
        "4. Kkma\n",
        "\n",
        "Mecab is faster than Kkma about 30~40 times\n",
        "Mecab is faster than Okt about 10 times\n",
        "Mecab is faster than Komoran about 5 times\n",
        "\"\"\"\n",
        "\n",
        "# Early versions use macab.\n",
        "MECAB = Mecab()\n",
        "\n",
        "# Learning target's name\n",
        "AI_TARGET_NAME = \"김경호\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtyggGY871Bm"
      },
      "source": [
        "# Funtion\n",
        "*   [Word2Vec Reference 1](https://ebbnflow.tistory.com/153)\n",
        "*   [Word2Vec Reference 2](https://monetd.github.io/python/nlp/Word-Embedding-Word2Vec-%EC%8B%A4%EC%8A%B5/#%ED%95%9C%EA%B5%AD%EC%96%B4-word2vec-%EB%A7%8C%EB%93%A4%EA%B8%B0)\n",
        "*   [PCA(sklearn) Reference](https://m.blog.naver.com/tjdrud1323/221720259834)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aaczn0GQ71N4"
      },
      "source": [
        "\"\"\"\n",
        "#################### Don't Used Functions (use in past) ####################\n",
        "# Function about time comparison\n",
        "def pos_times(taggers, tagger_name, texts) :\n",
        "    time_list = []\n",
        "    for tagger in taggers :\n",
        "        print(type(tagger))\n",
        "        \n",
        "        t1 = time.time()\n",
        "        for text in texts :\n",
        "            pprint(tagger.pos(text))\n",
        "        \n",
        "        print(\"###################################\")\n",
        "        t2 = time.time()\n",
        "        time_list.append(t2 - t1)\n",
        "\n",
        "    plt.figure(figsize=(10,8))\n",
        "    plt.bar(tagger_name, time_list, color=(0.4,0.7,0.5))\n",
        "    plt.title('Learning Time with .pos', fontsize=17)\n",
        "    plt.ylabel('total seconds')\n",
        "\n",
        "Ex. pos_times(trggerList, tragger's name, textList)\n",
        "\n",
        "# Sparse Representation(희소표현) Function\n",
        "# Tokennize and indexing before one-hot encoding\n",
        "def tokenization_indexing(context_list) :\n",
        "    for idx in range(0, len(context_list)) :\n",
        "        tmp_dic = {}\n",
        "        unique_idx = 1\n",
        "        for token in mecab.pos(context_list[idx]) :\n",
        "            tmp_dic[token[0]] = unique_idx\n",
        "            unique_idx += 1\n",
        "        \n",
        "        context_list[idx] = tmp_dic\n",
        "\n",
        "Ex. tokenization_indexing(dataset)\n",
        "\n",
        "# One-Hot encoding Function\n",
        "def onehot_encoding(dataset) :\n",
        "    oh_enc = OneHotEncoder()\n",
        "\n",
        "    oh_dataset = oh_enc.fit_transform(dataset.reshape(-1,1))\n",
        "    dataset = oh_dataset\n",
        "\n",
        "Ex. onehot_encoding(x_list)\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "#################### Data Extraction Functions ####################\n",
        "\"\"\"\n",
        "# Extract reply time (minute)\n",
        "def extract_replytime(content) :\n",
        "    end_idx = content.rfind(']')\n",
        "    start_idx = content[:end_idx].rfind('[') + 1\n",
        "\n",
        "    replytime = content[start_idx : end_idx]\n",
        "\n",
        "    if replytime[:2] == \"오전\" :\n",
        "        replytime = (int(replytime[2:replytime.index(\":\")]) * 60) + int(replytime[replytime.index(\":\") + 1 :])\n",
        "    else :\n",
        "        replytime = 60 * 12 + (int(replytime[2:replytime.index(\":\")]) * 60) + int(replytime[replytime.index(\":\") + 1 :])\n",
        "\n",
        "    return replytime\n",
        "\n",
        "# Comparison about reply time\n",
        "def compare_replytime(pre_text, cur_text) :\n",
        "    pre_time = extract_replytime(pre_text)\n",
        "    cur_time = extract_replytime(cur_text)\n",
        "\n",
        "    result = False\n",
        "    if cur_time - pre_time >= 10 : result = True\n",
        "\n",
        "    return result\n",
        "\n",
        "# Extract Data about kakao talk's content\n",
        "def extract_data(X_dataset, y_dataset) :\n",
        "    global AI_TARGET_NAME\n",
        "\n",
        "    tmp_X_list = []\n",
        "    tmp_Y_list = []\n",
        "    before_respondent = \"\"\n",
        "    for text in test.values :\n",
        "        text = text.tolist()\n",
        "        for t in text : \n",
        "            # Don't extract email\n",
        "            if (not \"--------------\" in t) and (not \"@\" in t) and  t.count(\":\") < 2 :\n",
        "                if not \"]\" in t : cur_respondent = before_respondent\n",
        "                else : cur_respondent = t[1:t.index(\"]\")]\n",
        "\n",
        "                if \":\" in t and before_respondent != \"\" :\n",
        "                    if compare_replytime(text[text.index(t) - 1], t) or cur_respondent != before_respondent :\n",
        "                        if len(tmp_X_list) != 0 : X_dataset.append(tmp_X_list)\n",
        "                        if len(tmp_Y_list) != 0 : y_dataset.append(tmp_Y_list)\n",
        "                        tmp_X_list = []\n",
        "                        tmp_Y_list = []\n",
        "\n",
        "                if AI_TARGET_NAME in t :\n",
        "                    ptext = t[(t.rfind(']') + 2) : len(t)]\n",
        "                    if ptext.find(\"http\") == -1 and ptext != \"사진\" and ptext != \"\" : \n",
        "                        before_respondent = AI_TARGET_NAME\n",
        "                        tmp_Y_list.append(ptext)\n",
        "                elif \"]\" in t :\n",
        "                    ptext = t[(t.rfind(']') + 2) : len(t)]\n",
        "                    if ptext.find(\"http\") == -1 and ptext != \"사진\" and ptext != \"\" : \n",
        "                        before_respondent = cur_respondent\n",
        "                        tmp_X_list.append(ptext)\n",
        "\n",
        "\"\"\"\n",
        "#################### Data manufacturing functions ####################\n",
        "\"\"\"\n",
        "# Factorization for reshaping's variable\n",
        "def factorization(x) :\n",
        "    common_factors = []\n",
        "    d = 2 \n",
        "    \n",
        "    while d <= x: \n",
        "        if x % d == 0: \n",
        "            common_factors.append(d)\n",
        "            x = x / d \n",
        "        else: d = d + 1\n",
        "    \n",
        "    return common_factors\n",
        "\n",
        "# Reshaping size on ndarray for training\n",
        "def reshape_sizing(X_dataset, y_dataset) :\n",
        "    X_dataset = X_dataset.reshape(X_sizing(X_dataset))\n",
        "    y_dataset = y_dataset.reshape(y_sizing(y_dataset))\n",
        "\n",
        "    return X_dataset, y_dataset\n",
        "\n",
        "def X_sizing(X_dataset) :\n",
        "    common_factors = factorization(X_dataset.shape[0]) # 공약수\n",
        "    step_size = common_factors[0] # timesteps\n",
        "    vector_length = common_factors[1] # n-length vectors\n",
        "\n",
        "    return int((X_dataset.shape[0] / step_size) / vector_length), step_size, vector_length\n",
        "\n",
        "def y_sizing(y_dataset) :\n",
        "    common_factors = factorization(y_dataset.shape[0])\n",
        "    dim_2 = common_factors[0] * common_factors[1]\n",
        "\n",
        "    return int(X_dataset.shape[0] / dim_2), dim_2\n",
        "\"\"\"\n",
        "#################### Auto Spacing functions ####################\n",
        "\"\"\"\n",
        "# Auto calibrate spacing functions\n",
        "def auto_spacing(X_dataset, y_dataset) :\n",
        "    calibrate_spcaing(X_dataset)\n",
        "    calibrate_spcaing(y_dataset)\n",
        "\n",
        "def calibrate_spcaing(context_dataset) :\n",
        "    global MECAB\n",
        "\n",
        "    for idx in range(0, len(context_dataset)) :\n",
        "        for text in context_dataset[idx] :\n",
        "            analyzedRes = MECAB.pos(text)\n",
        "            for mecabR in analyzedRes :\n",
        "                # \"MAG\" == Adverb\n",
        "                if mecabR[1] == \"MAG\" :\n",
        "                    try :\n",
        "                        startIdx = text.index(mecabR[0])\n",
        "                        if len(mecabR[0]) == 1 :\n",
        "                            if text[startIdx + 1] != \" \" and text[startIdx + 1].isalnum() : \n",
        "                                if text[startIdx + 2] != \" \" :\n",
        "                                    repairIdx = context_dataset[idx].index(text)\n",
        "                                    text = text[:startIdx] + text[startIdx] + \" \" + text[startIdx + 1 :]\n",
        "                                    context_dataset[idx][repairIdx] = text\n",
        "                        else : \n",
        "                            if text[startIdx + len(mecabR[0])] != \" \" and text[startIdx + len(mecabR[0])].isalnum() : \n",
        "                                if text[startIdx + len(mecabR[0]) + 1] != \" \" :\n",
        "                                    repairIdx = context_dataset[idx].index(text)\n",
        "                                    text = text[:startIdx] + text[startIdx : startIdx + len(mecabR[0])] + \" \" + text[startIdx + len(mecabR[0]) :]\n",
        "                                    context_dataset[idx][repairIdx] = text\n",
        "                    except IndexError :\n",
        "                        continue\n",
        "\n",
        "# Extraction 1 word by 1 content (use in Early version Model)\n",
        "def extract_1word(X_dataset, y_dataset) :\n",
        "    \"\"\" If don't use list, data type is str. this is caused just one word on word2vec \"\"\"\n",
        "    for idx in range(0, len(X_dataset)) : \n",
        "        X_dataset[idx] = X_dataset[idx][0].split(' ')\n",
        "\n",
        "    for idx in range(0, len(y_dataset)) : \n",
        "        y_dataset[idx] = y_dataset[idx][0].split(' ')\n",
        "\n",
        "\"\"\"\n",
        "#################### Stopword Function ####################\n",
        "\"\"\"\n",
        "# Set function and dataset for delete stopword\n",
        "def stopword_eraser(X_dataset, y_dataset) :\n",
        "    delete_stopword(X_dataset)\n",
        "    delete_stopword(y_dataset)\n",
        "\n",
        "# Delete stopword in sentence\n",
        "def delete_stopword(word_dataset) :\n",
        "    global MECAB\n",
        "\n",
        "    stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
        "    for idx in range(0, len(word_dataset)) :\n",
        "        tokenized_data = MECAB.morphs(word_dataset[idx][0])\n",
        "        tokenized_data = [word for word in tokenized_data if not word in stopwords]\n",
        "        word_dataset[idx] = tokenized_data\n",
        "\n",
        "\"\"\"\n",
        "#################### Word2Vec functions (Distributed(Dense) Representation) ####################\n",
        "\"\"\"\n",
        "# Draw a two-dimensional graph by entering the words, values of the two-dimensional X-axis, and values of the Y-axis.\n",
        "def plot_2d_graph(vocabs, xs, ys):\n",
        "    plt.figure(figsize=(8 ,6))\n",
        "    plt.scatter(xs, ys, marker = 'o')\n",
        "    for i, v in enumerate(vocabs):\n",
        "        plt.annotate(v, xy=(xs[i], ys[i]))\n",
        "\n",
        "# If x and y size different, equalize that\n",
        "def equalize_size(X_dataset, y_dataset) :\n",
        "    if len(X_dataset) > len(y_dataset) : X_dataset = X_dataset[1 : len(y_dataset) + 1]\n",
        "    elif len(y_dataset) > len(X_dataset) : y_dataset = y_dataset[1 : len(X_dataset) + 1]\n",
        "\n",
        "    return X_dataset, y_dataset\n",
        "\n",
        "# Conversioning dataset to word2vec format\n",
        "def dataset_to_word2vec(X_dataset, y_dataset) :\n",
        "    return word2vec(X_dataset, 'X'), word2vec(y_dataset, 'y')\n",
        "\n",
        "def word2vec(word_dataset, T) :\n",
        "    # Init words and vectors\n",
        "    w2v = Word2Vec(word_dataset, size=100, window=3, min_count=1, workers=6, sg=1)\n",
        "\n",
        "    # Set word vectors\n",
        "    word_vectors = w2v.wv\n",
        "    vocabs = word_vectors.vocab.keys()\n",
        "    word_vector_list = [word_vectors[v] for v in vocabs]\n",
        "\n",
        "    # Confirm word similarity\n",
        "    # if \"경호\" in vocabs : print(word_vectors.most_similar(\"경호\"))\n",
        "    # else : print(word_vectors.most_similar(\"대석\"))\n",
        "    \n",
        "    pca = PCA(n_components=2)\n",
        "    xys = pca.fit_transform(word_vector_list)\n",
        "    xs = xys[:,0]\n",
        "    ys = xys[:,1]\n",
        "\n",
        "    # plot_2d_graph(vocabs, xs, ys)\n",
        "    return xys.reshape(-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxjmmKaVlO9p"
      },
      "source": [
        "#Main\n",
        "*   [Dataset Classification Reference 1](https://ganghee-lee.tistory.com/38)\n",
        "*   [Dataset Classification Reference 2](https://ysyblog.tistory.com/69)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tk3zCKMclQz2"
      },
      "source": [
        "\"\"\" \n",
        "ERROR 3. ParserError: Error tokenizing data. C error \n",
        "- Solution: add code in read_csv = , sep='\\t'\n",
        "\"\"\"\n",
        "test = pd.read_csv('KKH_20200129~20210725.txt', sep='\\t')\n",
        "\n",
        "# 0. Init Dataset\n",
        "X_dataset = []\n",
        "y_dataset = []\n",
        "\n",
        "# 1. Data extraction\n",
        "extract_data(X_dataset, y_dataset)\n",
        "\n",
        "# 2. Space cailbrating\n",
        "auto_spacing(X_dataset, y_dataset)\n",
        "\n",
        "# 3. Data manufacturing\n",
        "# Early version is used y_list[][0] and x_list[][0] (1 word) by learning model\n",
        "extract_1word(X_dataset, y_dataset)\n",
        "# Delete stopword\n",
        "stopword_eraser(X_dataset, y_dataset)\n",
        "# Data type change on np.ndarray\n",
        "X_dataset = np.asarray(X_dataset).astype(object)\n",
        "y_dataset = np.asarray(y_dataset).astype(object)\n",
        "# Word2Vec on dataset\n",
        "X_dataset, y_dataset = dataset_to_word2vec(X_dataset, y_dataset)\n",
        "# Size equalize\n",
        "X_dataset, y_dataset = equalize_size(X_dataset, y_dataset)\n",
        "# Dataset reshape\n",
        "X_dataset, y_dataset = reshape_sizing(X_dataset, y_dataset)\n",
        "# Data division (Train : Validation : Test = 6 : 2 : 2)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_dataset, y_dataset,\n",
        "                                                    test_size = 0.2)\n",
        "print(\"########## Train + Validation (X,) (y,) / Test (X,) (y,) ##########\")\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n",
        "                                                  test_size = 0.2)\n",
        "print(\"########## Train (X,) (y,) / Validation (X,) (y,) ##########\")\n",
        "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)\n",
        "\n",
        "# 4. Modeling\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters=64, kernel_size=2, activation=\"relu\", padding='same', input_shape=(X_train.shape[1:])))\n",
        "model.add(MaxPooling1D(pool_size=2, padding='same'))\n",
        "model.add(Conv1D(filters=64, kernel_size=2, activation=\"relu\", padding='same'))\n",
        "model.add(MaxPooling1D(pool_size=2, padding='same'))\n",
        "model.add(Conv1D(filters=64, kernel_size=2, activation=\"relu\", padding='same'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation=\"relu\"))\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "model.compile(loss='mse', optimizer=Adam(), metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# 5. Model learning (fit)\n",
        "model.fit(X_train, y_train,\n",
        "          validation_data = (X_val, y_val),\n",
        "          batch_size = X_train.shape[0],\n",
        "          epochs = 1,\n",
        "          verbose = 2)\n",
        "learning_lost, learning_acc = model.evaluate(X_train, y_train, verbose=2)\n",
        "print(\"Learning accuracy :\", learning_acc)\n",
        "print(\"Learning loss % :\", learning_lost)\n",
        "\n",
        "# 6. Aengmu operate\n",
        "Aengmu = model.predict(X_test)\n",
        "print(vec2word(Aengmu[0], 'y'))\n",
        "print(vec2word(y_test[0], 'y'))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
